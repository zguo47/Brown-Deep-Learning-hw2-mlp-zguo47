{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "65e32efb-34fd-47c4-8614-b75587650151",
   "metadata": {},
   "source": [
    "# CS1470/2470 HW2: Multi-Layered Neural Networks\n",
    "\n",
    "In this homework assignment, you will build a sequential model using differential modules.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c37151f-a883-4c0b-a09a-b68f5ff1bd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "%aimport Beras, assignment, preprocess"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e9d8351-fb6b-4e23-b9cd-7d7a951f8cd3",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Pull In The Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "369b71e0-092e-4ad1-9d63-ee4fcc952a29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ![ ! -d '../../data' ] && bash cd ../.. && bash download.sh\n",
    "data_path = \"../data\"\n",
    "\n",
    "## Import MNIST train and test examples into train and testing data\n",
    "import preprocess\n",
    "X0, Y0 = preprocess.get_data_MNIST('train', data_path)\n",
    "X1, Y1 = preprocess.get_data_MNIST('test',  data_path)\n",
    "\n",
    "print(\"Training Shapes:\", X0.shape, Y0.shape)\n",
    "print(\"Testing  Shapes:\", X1.shape, Y1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c2bd75-32c9-43ff-b74d-9ce3a470e473",
   "metadata": {},
   "source": [
    "**> Expected Output** (double-click)\n",
    "<!-- \n",
    "```\n",
    "Training Shapes: (60000, 784) (60000,)\n",
    "Testing  Shapes: (10000, 784) (10000,)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cc0c2a1-f97a-4233-9612-bfecf93dd0eb",
   "metadata": {},
   "source": [
    "## Starting Our Modular API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a20c25c8-5f1a-47c5-8ebd-28c8e6c85a20",
   "metadata": {},
   "source": [
    "### **The goal of this assignment is as follows:** \n",
    "- Extend our knowledge of deep learning by extending our single-layer model intuitions into multi-layer extensions. \n",
    "- Get familiarized with a simple modular API which reflects (but is a simplification of) the Keras analogue. \n",
    "    - Specifically, the `SequentialModel`\n",
    "- Implement some nice modular components and be able to construct a functional single-file neural network from it. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e4e2f2d-cf52-4a13-bd51-ec697c3ab874",
   "metadata": {},
   "source": [
    "### Improving The Old Way via Chain Rule\n",
    "\n",
    "Recall that in the machine learning lab, we got the chance to make some simple but effective regression models! Given some realizations $X \\sim \\mathcal{X}$ and $Y = \\mathbb{E}[Y|X] + \\xi$, we were able to train up a model $h_{\\theta} \\in \\mathcal{H}$ which was similar to $\\mathbb{E}[Y|X]$ and thereby minimized an empirical loss $\\mathcal{L}$ of our choice. \n",
    "\n",
    "In these cases, we assumed that $h_{\\theta}$ had a relatively simple and non-flexible architecture, which meant that we could manually specify the structure and simply derive and code up our gradient formula once. Furthermore, since the optimization process was concave, we could even skip the gradient computation and directly derive a loss-minimizing parameter selection. \n",
    "\n",
    "Of course, there are hard limits to what this kind of architecture can provide us; sometimes the relationships that the model needs to capture are relatively complex and might not be resolvable in such a fashion. And that's why this course exists!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1235cf7-906c-40d4-b360-c2ffccd7a0f4",
   "metadata": {},
   "source": [
    "**Naive Solution:** If we really wanted to, we could approach this problem in the same way as before, and just code up the gradient functions manually. Similarly to before, this would allow us to propagate gradients through, say, a specified loss function, an activation function, and a dense layer. Similarly, we could do it for 2 dense layers; just use the old gradient function for the weights in layer 2, compute the new gradient for the weights in layer 1, and so on. \n",
    "\n",
    "**Problem:** This is extremely time-consuming and rigid! What happens if we switched out an activation function? A loss function? We'd have to re-specify the gradient every time!\n",
    "\n",
    "**Solution:** Let's take advantage of the chain rule! \n",
    "\n",
    "Recall that per the chain rule, if there exists a set of differentiable functions $c(b)$ and $b(a)$, then \n",
    "\n",
    "$$\\frac{\\partial a}{\\partial c} = \\frac{\\partial a}{\\partial b} \\frac{\\partial b}{\\partial c}$$\n",
    "\n",
    "Going back to our regression model, let's assume that we have a layered process: \n",
    "\n",
    "$$x \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
    "\n",
    "This implies that we can compute the partial of the trainable parameters $\\theta$ through a loss evaluation $\\mathcal{L}$ and a dense layer $h_\\theta(x)$ by the following relationship:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial \\mathcal{h_\\theta}}{\\partial \\theta}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
    "\n",
    "With a similar logic, you can also make the assertion that you can also get the partial with respect to the input $x$:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h_\\theta}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h_\\theta}$$\n",
    "\n",
    "So... by the same token, is there anything stopping us from going further? Let's say that we decided to have another hypothesis function such that $x = h'_{\\theta'}(x')$ for some other hypothesis function and inputs? The new structure would then be: \n",
    "\n",
    "$$x' \\to \\big[ x = h'_{\\theta'}(x') \\big] \\to h_\\theta(x) \\to \\mathcal{L}(h_\\theta)$$\n",
    "\n",
    "Without the chain rule, coding in the facilities to optimize $\\theta'$ might have been tricky, but with the chain rule we know that:\n",
    "\n",
    "$$\\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
    "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial h}{\\partial x}\\frac{\\partial \\mathcal{L}}{\\partial h} \n",
    "= \\frac{\\partial x}{\\partial \\theta'}\\frac{\\partial \\mathcal{L}}{\\partial x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03450d0c-e0af-4665-bf15-c550a226482b",
   "metadata": {},
   "source": [
    "Notice how this process is both predictable and scales very well! Say that we wanted to add some activation functions to restrict the range of the hypothesis functions. This trivially inserts into the chain and everything still works and will look something like this: \n",
    "\n",
    "$$ \\frac{\\partial \\mathcal{L}}{\\partial x} = \\frac{\\partial h}{\\partial x}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
    "\\ \\text{ and } \\ \\frac{\\partial \\mathcal{L}}{\\partial \\theta} = \\frac{\\partial h}{\\partial \\theta}\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e88624a-3ea3-483f-b1a3-75b57e326872",
   "metadata": {},
   "source": [
    "And with that, we start to approach the reason why this is such a powerful formulation: The cumulative nature of the process. Specifically, consider the process that needs to happen in order to compute this for the extended 2-layer example: \n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\frac{\\partial a}{\\partial h}\\frac{\\partial \\mathcal{L}}{\\partial a} \n",
    "= \\frac{\\partial \\mathcal{L}}{\\partial h} \n",
    "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x} \n",
    "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'}\n",
    "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
    "&\\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
    "\\\\\n",
    "&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
    "&&&\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
    "% \\\\\n",
    "% &\\searrow \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta} \n",
    "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial a'} \n",
    "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial h'} \n",
    "% &\n",
    "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial \\theta'} \n",
    "% \\\\ \n",
    "% &&\n",
    "% \\to \\cdots = \\frac{\\partial \\mathcal{L}}{\\partial x'} \n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "... and this is the process known as **back-propagation** *(and a special case of **auto-differentiation**)*!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a945cbd-4903-4f13-9693-b864e871a736",
   "metadata": {},
   "source": [
    "### **Exploring a possible modular implementation: TensorFlow/Keras**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e732371d",
   "metadata": {},
   "source": [
    "We can check out what an established deep learning framework does to help us motivate our model. You'll learn more about this in the TensorFlow lab, but you can define a deep learning architecture using, among many other options, the `SequentialModel`:\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.optimizers import Adam                  ## Special\n",
    "from tensorflow.keras.layers import Dense, LeakyReLU, Softmax ## Differentiable\n",
    "from tensorflow.keras.losses import MeanSquaredError          ## Differentiable\n",
    "from tensorflow.keras.metrics import MeanSquaredError         ## Non-Differentiable (but Callable)\n",
    "from tensorflow.keras import Sequential                       ## Differentiable (surprisingly enough)\n",
    "\n",
    "## Simple Model Architecture\n",
    "tf.keras.Sequential([\n",
    "    Dense(32),    ## ? -> 32 units per entry\n",
    "    LeakyReLU(),  ## Bind logits to [min(l)*alpha, max(l)]\n",
    "    Dense(1),     ## 4 -> 1 units per entry\n",
    "    Softmax()     ## Bind logits to [0, 1] such that sum(logits) = 1\n",
    "])\n",
    "\n",
    "model.compile(\n",
    "    optimizer   = Adam(learning_rate=1),\n",
    "    loss        = MeanSquaredError(),\n",
    "    metrics     = [MeanSquaredError()]\n",
    ")\n",
    "\n",
    "model.fit(X0, Y0, batch_size = 20, epochs = 10)\n",
    "model.evaluate(X0, Y0, batch_size = 100)\n",
    "```\n",
    "\n",
    "This shows the main Keras value proposition: Modular components which you can move around and customize to your heart's content. This allows for rapid prototyping and small code bases, which allows almost anybody to get started with deep learning!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51eaf3e1",
   "metadata": {},
   "source": [
    "Keras itself is made up of building blocks from (and is a well-defined part of) Tensorflow. \n",
    "- **Keras:** Gives you high-level building blocks to keep track of internal components and train models easily.\n",
    "- **Tensorflow**: Gives you the lower-level building blocks to easily differentiate and perform math operations. \n",
    "\n",
    "You'll find out more about this in the lab, but you can get a feel for how Tensorflow can be used by digging into the more low-level implementation of the Sequential Model *(which still uses Keras... I mean, we're not barbarians, right?)*: "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93eda366",
   "metadata": {},
   "source": [
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "class SequentialModel(tf.keras.Model):\n",
    "    '''\n",
    "    Note, this is a simplification, but it's close enough...\n",
    "    It's also not exactly what you'll be doing (but really close)\n",
    "    Note that a lot of things are missing: That's because they're handled by tf.keras.Model\n",
    "    '''\n",
    "    def __init__(self, layers):\n",
    "        ## Phase at which you should specify the variables needed to do passes\n",
    "        self.layers = layers\n",
    "\n",
    "    def call(self, inputs, training=True):\n",
    "        ## Phase at which you specify forward propagation\n",
    "        x = tf.identity(inputs)   ## Copy input to de-reference it\n",
    "        for layer in layers: \n",
    "            x = layer(x)\n",
    "        return x\n",
    "\n",
    "    def train_step(self, data):\n",
    "        ## Optional Train_Step Specification\n",
    "        ## This thing gets called for every batch\n",
    "        x, y = data\n",
    "        with tf.GradientTape() as tape:  ## While in scope of gradient tape: \n",
    "            ## Record what happens between all tf.Variable operations and \n",
    "            ## keep track of the partial gradients that get generated.\n",
    "            logits  = self.call(x)          ## This is differentiable (i.e. implemented w/ tf.Variables)\n",
    "            loss    = self.loss(y, logits)  ## This is also differentiable\n",
    "        \n",
    "        ## Figure out dL/dw for every trainable weights w based on \n",
    "        ## which operations were performed between them.\n",
    "        grads = tape.gradient(loss, self.trainable_weights)\n",
    "        optimizer.apply_gradients(zip(grads, self.trainable_weights))\n",
    "\n",
    "        # Update metrics (includes the metric that tracks the loss)\n",
    "        self.compiled_metrics.update_state(y, logits)\n",
    "        # Return a dict mapping metric names to current value\n",
    "        return {m.name: m.result() for m in self.metrics}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea829e5e",
   "metadata": {},
   "source": [
    "So... make it! ... kinda...\n",
    "\n",
    "...yeah, that's a tall order. A few things to not about this:\n",
    "- Tensorflow uses tf.Variables to do auto-differentiation from the operator level onward, which includes support for basic processes like addition and quotients and a bunch of other things. That's way too much detail for us to implement! \n",
    "- Real models oftentimes have variables and representations that diverge paths before reconvening into loss evaluations. In order to handle this, you'd need to implement a graph structure. This would just distract us from the main focus.\n",
    "\n",
    "The sequential model allows us to simplify both of these away: \n",
    "- Since we only use large modules (i.e. Dense, ReLU, Sigmoid, MeanSquaredError, etc.), we can implement differentiation on that level: This will prove to be surprisingly tractible – but not exactly trivial, hence the assignment.\n",
    "- Since the sequential model by default assumes a one-module-at-a-time pass-through, we don't have to worry too much about implicitly supporting primitive operations that cause sophisticated branching. \n",
    "    - Most components will only require one pathway in back-propagation: towards the partial with respect to its inputs. \n",
    "    - The dense layer will require the most amount of branching among our (1470) components, since it requires partials with respect to inputs, weights, and biases. We can handle that, right...?\n",
    "    \n",
    "Also note that the point of this exercise is not to implement a real library, nor is it to implement TF/Keras faithfully. As such, there is little consideration for realistic optimization (aside from vectorization) or scale beyond the assignment.\n",
    "\n",
    "A more realistic (though still very pedagogical) implementation named [Brunoflow](https://github.com/Brown-Deep-Learning/brunoflow) is available from our very own Daniel Ritchie! It is a really nice library, but unfortunately is a bit too much for students to implement (and is also publicly available). Feel free to look at it whenever you want, as the two versions are very different. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db05df1-6889-4280-ab95-48109596feb4",
   "metadata": {},
   "source": [
    "### Testing out our first Callable module: **OneHotEncoder**\n",
    "\n",
    "To get you all warmed up to the idea of the modules, we have already provided you with an implementation of the `OneHotEncoder`.\n",
    "\n",
    "- **`OneHotEncoder`**\n",
    "    - **Pre-processing step:** Takes label dataset and converts each label $\\ell \\in L$ such that $\\text{ohe}(\\ell) = \\mathbb{1}_{i=0}^{\\|L\\|}(i = \\ell)$\n",
    "    - **Callable:** Does not need to be differentiated, since not in optimization loop.\n",
    "    - **Stateful:** Has to keep track of some states. For example, the set of unique elements and their mappings to the one-hot vectors.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42bfb7f9-b31b-462a-96f9-ee9981949a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport Beras\n",
    "\n",
    "# from Beras.preprocess import OneHotEncoder\n",
    "# from Beras.core import Callable\n",
    "\n",
    "from abc import ABC, abstractmethod  ## For abstract method support\n",
    "import numpy as np\n",
    "\n",
    "class Callable:\n",
    "    \n",
    "    def __call__(self, *args, **kwargs):\n",
    "        '''Lets `self()` and `self.forward()` be the same'''\n",
    "        return self.forward(*args, **kwargs)\n",
    "\n",
    "    @abstractmethod\n",
    "    def forward(self, *args, **kwargs):\n",
    "        \"\"\"Propagates input through the network. Stores inputs and outputs as instance variables\"\"\"\n",
    "        pass\n",
    "\n",
    "class OneHotEncoder(Callable):\n",
    "    '''\n",
    "    One-Hot Encodes labels. First takes in a fit-set to figure out what elements it \n",
    "    needs to consider, and then one-hot encodes subsequent input datasets in the \n",
    "    forward pass. \n",
    "    \n",
    "    SIMPLIFICATIONS: \n",
    "     - Implementation assumes that entries are individual elements.\n",
    "     - Forward will call fit if it hasn't been done yet; most implementations will just error.\n",
    "     - keras does not have OneHotEncoder; has LabelEncoder, CategoricalEncoder, and to_categorical()\n",
    "    '''  \n",
    "    def fit(self, data):\n",
    "        '''\n",
    "        Fits the one-hot encoder to a candidate dataset. Said dataset should contain \n",
    "        all encounterable elements.\n",
    "        '''\n",
    "        self.uniq = np.unique(data)\n",
    "        self.vecs = np.eye(len(self.uniq))\n",
    "        self.uniq2oh = {e : self.vecs[i] for i, e in enumerate(self.uniq)} \n",
    "\n",
    "    def forward(self, data):\n",
    "        if not hasattr(self, 'uniq2oh'): self.fit(data)\n",
    "        return np.array([self.uniq2oh[x] for x in data])\n",
    "\n",
    "    def inverse(self, data):\n",
    "        assert hasattr(self, 'uniq'), 'forward() or fit() must be called before attempting to invert'\n",
    "        return np.array([self.uniq[x == 1][0] for x in data])\n",
    "\n",
    "ohe = OneHotEncoder()\n",
    "ohe.fit(np.concatenate([Y0, Y1], axis=-1))\n",
    "\n",
    "print(\"Getting label sample:\")\n",
    "print(Y0[:10])\n",
    "print(\"Testing Forward:\")\n",
    "print(''.join([f'  {i}' for i in range(10)]), 'is hot')\n",
    "print(ohe(Y0[:10]))\n",
    "print(\"Testing Inverse:\")\n",
    "print(ohe.inverse(ohe(Y0[:10])))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97422d2c-e80e-4556-a479-164eefaf050b",
   "metadata": {},
   "source": [
    "**> Expected Output**\n",
    "<!-- \n",
    "```\n",
    "Getting label sample:\n",
    "[5 0 4 1 9 2 1 3 1 4]\n",
    "Testing Forward:\n",
    "  0  1  2  3  4  5  6  7  8  9 is hot\n",
    "[[0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
    " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
    " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
    " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]]\n",
    "Testing Inverse:\n",
    "[5 0 4 1 9 2 1 3 1 4] \n",
    "```-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0791f711-6313-4d66-91d1-2807c68fb308",
   "metadata": {},
   "source": [
    "### Testing out our first Diffable module: **Categorical Cross-Entropy**\n",
    "\n",
    "An extension of Callable – a module that can be called – is the Diffable – a module that can be Diff(erentiat)ed. The details are quite... specific. We (or at least Vadim) originally wanted you all to do this, but then we realized that this would take an extremely long amount of time to implement and debug even if you already knew all of the concepts. So instead, we've provided the source code for this whole part (in fact, almost all of `Beras/core.py`) in the assignment stencil. Sometimes it's better to have you struggle to reason about a well-debugged implementation than to actually have you implement it yourself... or at least that's the mindset we're going with. Hope you appreciate it :)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e18d4cd0",
   "metadata": {},
   "source": [
    "```python\n",
    "class Diffable(ABC, Callable):\n",
    "\n",
    "    '''Is the operation being used inside a gradient tape scope?'''\n",
    "    gradient_tape = None    ## All-instance-shared variable\n",
    "\n",
    "    def __call__(self, *args, **kwargs):\n",
    "        '''\n",
    "        If there is a gradient tape scope in effect, perform AND RECORD the operation.\n",
    "        Otherwise... just perform the operation and don't let the gradient tape know. \n",
    "        '''\n",
    "\n",
    "    @abstractmethod\n",
    "    def input_gradients(self):\n",
    "        \"\"\"Returns gradient for input (this part gets specified for all diffables)\"\"\"\n",
    "\n",
    "    def weight_gradients(self):\n",
    "        \"\"\"Returns gradient for input (this part gets specified for SOME diffables)\"\"\"\n",
    "\n",
    "    def compose_to_input(self, J):\n",
    "        \"\"\"\n",
    "        Compose the inputted cumulative jacobian with the input jacobian for the layer.\n",
    "        Implemented with batch-level vectorization.\n",
    "\n",
    "        Requires `input_gradients` to provide either batched or overall jacobian.\n",
    "        Assumes input/cumulative jacobians are matrix multiplied\n",
    "        \"\"\"\n",
    "\n",
    "    def compose_to_weight(self, J):\n",
    "        \"\"\"\n",
    "        Compose the inputted cumulative jacobian with the weight jacobian for the layer.\n",
    "        Implemented with batch-level vectorization.\n",
    "\n",
    "        Requires `weight_gradients` to provide either batched or overall jacobian.\n",
    "        Assumes weight/cumulative jacobians are element-wise multiplied (w/ broadcasting)\n",
    "        and the resulting per-batch statistics are averaged together for avg per-param gradient.\n",
    "        \"\"\"\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9bb6b639",
   "metadata": {},
   "source": [
    "#### **Simple Diffable Example:** Sin Activation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea72739a",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport Beras\n",
    "\n",
    "from Beras.core import Diffable\n",
    "import numpy as np\n",
    "\n",
    "class Sin(Diffable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.inputs = None\n",
    "        self.outputs = None  ## optional in this case\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        self.inputs = inputs\n",
    "        return np.sin(inputs)\n",
    "\n",
    "    def input_gradients(self):\n",
    "        return np.cos(self.inputs)\n",
    "        \n",
    "    def compose_to_input(self, J):\n",
    "        ## One of the few times you may want to do this.\n",
    "        ## We'll leave it up to you to figure out when.\n",
    "        return self.input_gradients() * J\n",
    "\n",
    "act_fn = Sin()\n",
    "## 2 batches with 3 entries and 4 elements per entry\n",
    "sample = np.arange(-12, 12).reshape(2, 3, 4)       \n",
    "out = act_fn(sample)\n",
    "\n",
    "print(\"Activation Input:\")\n",
    "print(sample)\n",
    "\n",
    "print(\"\\nActivation Output:\")\n",
    "print(out)\n",
    "\n",
    "print(\"\\nInput Gradients:\")\n",
    "print(act_fn.input_gradients())\n",
    "\n",
    "print(\"\\nCompose To Input:\")\n",
    "print(act_fn.compose_to_input(out))\n",
    "\n",
    "print(\"\\nSanity Check:\")\n",
    "comp1 = act_fn.compose_to_input(act_fn.input_gradients())\n",
    "comp2 = act_fn.input_gradients() * act_fn.input_gradients()\n",
    "if np.allclose(comp1, comp2):\n",
    "    print(\"The world still makes sense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5c26ff",
   "metadata": {},
   "source": [
    "**> Expected Output**\n",
    "<!-- \n",
    "```\n",
    "Activation Input:\n",
    "[[[-12 -11 -10  -9]\n",
    "  [ -8  -7  -6  -5]\n",
    "  [ -4  -3  -2  -1]]\n",
    "\n",
    " [[  0   1   2   3]\n",
    "  [  4   5   6   7]\n",
    "  [  8   9  10  11]]]\n",
    "\n",
    "Activation Output:\n",
    "[[[ 0.53657292  0.99999021  0.54402111 -0.41211849]\n",
    "  [-0.98935825 -0.6569866   0.2794155   0.95892427]\n",
    "  [ 0.7568025  -0.14112001 -0.90929743 -0.84147098]]\n",
    "\n",
    " [[ 0.          0.84147098  0.90929743  0.14112001]\n",
    "  [-0.7568025  -0.95892427 -0.2794155   0.6569866 ]\n",
    "  [ 0.98935825  0.41211849 -0.54402111 -0.99999021]]]\n",
    "\n",
    "Input Gradients:\n",
    "[[[ 0.84385396  0.0044257  -0.83907153 -0.91113026]\n",
    "  [-0.14550003  0.75390225  0.96017029  0.28366219]\n",
    "  [-0.65364362 -0.9899925  -0.41614684  0.54030231]]\n",
    "\n",
    " [[ 1.          0.54030231 -0.41614684 -0.9899925 ]\n",
    "  [-0.65364362  0.28366219  0.96017029  0.75390225]\n",
    "  [-0.14550003 -0.91113026 -0.83907153  0.0044257 ]]]\n",
    "\n",
    "Compose To Input:\n",
    "[[[ 0.45278918  0.00442565 -0.45647263  0.37549362]\n",
    "  [ 0.14395166 -0.49530368  0.26828646  0.27201056]\n",
    "  [-0.49467912  0.13970775  0.37840125 -0.45464871]]\n",
    "\n",
    " [[ 0.          0.45464871 -0.37840125 -0.13970775]\n",
    "  [ 0.49467912 -0.27201056 -0.26828646  0.49530368]\n",
    "  [-0.14395166 -0.37549362  0.45647263 -0.00442565]]]\n",
    "\n",
    "Sanity Check:\n",
    "The world still makes sense\n",
    "```-->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b426ddaf",
   "metadata": {},
   "source": [
    "#### **Advanced Diffable Example:** Categorical Crossentropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "208a0ba1-a1e5-45e5-9a1a-44cb94a29594",
   "metadata": {},
   "outputs": [],
   "source": [
    "%aimport Beras\n",
    "\n",
    "from Beras.core import Diffable\n",
    "import numpy as np\n",
    "\n",
    "## Special consideration to avoid multiplying by negative infinity\n",
    "def clip_0_1(x, eps=1e-6):\n",
    "    return np.clip(x, eps, 1-eps)\n",
    "        \n",
    "class CategoricalCrossentropy(Diffable):\n",
    "\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        ## Initialize states to be None; wait for them to be populated.\n",
    "        self.truths  = None\n",
    "        self.inputs  = None\n",
    "        self.outputs = None\n",
    "\n",
    "    def forward(self, inputs, truths):\n",
    "        ll_right =      truths  * np.log(clip_0_1(    inputs))\n",
    "        ll_wrong = (1 - truths) * np.log(clip_0_1(1 - inputs))\n",
    "        nll_total = -np.mean(ll_right + ll_wrong, axis=-1)\n",
    "\n",
    "        ## Notice the state tracking. \n",
    "        ## This is because some gradient computations require the input, \n",
    "        ## the output, or even some other things.\n",
    "        self.inputs = inputs\n",
    "        self.truths = truths\n",
    "        self.outputs = np.mean(nll_total, axis=0)\n",
    "        return self.outputs\n",
    "\n",
    "    def input_gradients(self):\n",
    "        bn, n = self.inputs.shape\n",
    "        grad = np.zeros(shape=(bn, n), dtype=self.inputs.dtype)\n",
    "        for b in range(bn):\n",
    "            ## Notice the per-batch separation. This is because each batch \n",
    "            ## should get its own jacobian matrix, which facilitates \n",
    "            ## batch-average gradient updates. \n",
    "            inp = self.inputs[b]\n",
    "            tru = self.truths[b]\n",
    "            grad[b] = inp - tru\n",
    "            grad[b] /= clip_0_1(inp - inp**2)\n",
    "            grad[b] /= inp.shape[-1]\n",
    "        return grad\n",
    "    \n",
    "loss_fn = CategoricalCrossentropy()\n",
    "## 3 batches with 3 elements per batch (losses, right?)\n",
    "ones = np.ones((3, 3)) \n",
    "zeros = np.zeros_like(ones)\n",
    "\n",
    "print(\"Awful performance\")\n",
    "print(loss_fn(ones, zeros))\n",
    "print(loss_fn(zeros, ones))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nPerfect performance\")\n",
    "print(loss_fn(ones, ones))\n",
    "print(loss_fn(zeros, zeros))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nNot great performance\")\n",
    "print(loss_fn(ones * 0.5, ones))\n",
    "print(\"Input Gradients:\")\n",
    "print(loss_fn.input_gradients())\n",
    "\n",
    "print(\"\\nWeight Gradients:\")\n",
    "print(loss_fn.weight_gradients())\n",
    "\n",
    "print(\"\\nCompose To Input:\")\n",
    "print(loss_fn.compose_to_input(np.eye(3)))\n",
    "\n",
    "print(\"\\nSanity Check:\")\n",
    "comp1 = loss_fn.compose_to_input(loss_fn.input_gradients())\n",
    "comp2 = loss_fn.input_gradients() @ loss_fn.input_gradients()\n",
    "if np.allclose(comp1, comp2):\n",
    "    print(\"The world still makes sense\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fe061a6",
   "metadata": {},
   "source": [
    "**> Expected Output**\n",
    "<!-- \n",
    "```\n",
    "Awful performance\n",
    "13.815510557964274\n",
    "13.815510557964274\n",
    "Input Gradients:\n",
    "[[-333333.33333333 -333333.33333333 -333333.33333333]\n",
    " [-333333.33333333 -333333.33333333 -333333.33333333]\n",
    " [-333333.33333333 -333333.33333333 -333333.33333333]]\n",
    "\n",
    "Perfect performance\n",
    "1.000000500029089e-06\n",
    "1.000000500029089e-06\n",
    "Input Gradients:\n",
    "[[0. 0. 0.]\n",
    " [0. 0. 0.]\n",
    " [0. 0. 0.]]\n",
    "\n",
    "Not great performance\n",
    "0.6931471805599453\n",
    "Input Gradients:\n",
    "[[-0.66666667 -0.66666667 -0.66666667]\n",
    " [-0.66666667 -0.66666667 -0.66666667]\n",
    " [-0.66666667 -0.66666667 -0.66666667]]\n",
    "\n",
    "Weight Gradients:\n",
    "()\n",
    "\n",
    "Compose To Input:\n",
    "[[-0.66666667 -0.66666667 -0.66666667]\n",
    " [-0.66666667 -0.66666667 -0.66666667]\n",
    " [-0.66666667 -0.66666667 -0.66666667]]\n",
    "\n",
    "Sanity Check:\n",
    "The world still makes sense\n",
    "```-->"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "759be6693a164ddeab1e231298c2a01a8302a7c7dfd4e560844dbce42a896f34"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
